 Uljahn: :v:
 tutubalin: эммм... ивент будет командный
 tutubalin: это значит опять ескейп?
 tutubalin: а, или под командой они подразумевают компанию
 Uljahn: да, теперь можно участвовать в команде, её надо регать, и назначать админа команды, вроде
 TheCrucial: это чисто для того чтобы был какой то "командный зачет"? и заодно одинаковые "командные" одинаковые решения не банились
 Uljahn: одинаковые решения? думаю, будут баниться, иначе смысл командного зачёта теряется
 Uljahn: хотя, детект одинаковости вроде только для леги делают
 TheCrucial: я имею ввиду что если внутри команды пересечения - можно не банить. или это плохая идея?
 Uljahn: пересечения можно не банить, наверное, а вот 100% копии лучше банить, причём обе
 inoryy: Uljahn такие расчеты про стоимость обучение больших моеделей обычно очень мимо реальности.
 Uljahn: конечно, это для дяди "со стороны" расчёты, у крупных компаний база своя, выходит гораздо дешевле, я думаю
 Uljahn: у них и компетентные спецы, и биг дата, и железо
 Uljahn: GTP-3 же пытаются как-то монетизировать, с DALL-E наверное так же будет
 Uljahn: можно из стоимости монетизации исходить
 inoryy: ну я соавтор одной из таких моделей (Gopher 280B), даже для "дяди со стороны" там рассчеты не очень
 Uljahn: не очень затратные?
 inoryy: меньше чем люди думают; ну если не уметь оптимизировать то всегда можно бюджет взорвать конечно; ну и вслепую скалировать тоже так себе занятие, наша 70B значительно сильнее 530B от Nvidia
 Uljahn: <a rel="nofollow" class="external free" href="https://blog.heim.xyz/palm-training-cost">https://blog.heim.xyz/palm-training-cost</a> я вот эту читал, довольно свежая, хотя и натяжек достаточно
 Uljahn: Gopher там тоже упоминается
 inoryy: <a rel="nofollow" class="external free" href="https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training">https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training</a>
 Uljahn: да это-то понятно, большая не значит лучшая
 Uljahn: Andrew Ng недавно в таком же ключе высказывался, как бы не прибили его
